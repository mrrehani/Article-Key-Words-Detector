{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is from https://www.kaggle.com/yufengdev/bbc-fulltext-and-category?select=bbc-text.csv\n",
    "articles = pd.read_csv(os.path.join(os.getcwd(), \"Articles.csv\")) \n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "newStopWords = ['said','also',\"one\",\"two\",\"three\",\"four\",\"five\",'u','would',\"second\",\"first\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",'sunday','mr',\"mrs\",'ms',\"name\"]\n",
    "stop.extend(newStopWords)\n",
    "def word_count(identifier, individual=False):\n",
    "    #The next 4 lines of code are from https://www.kaggle.com/edhirif/word-cloud-alternative-using-nltk\n",
    "    if individual==False:\n",
    "        article_str = articles[articles[\"category\"]==identifier][\"text\"].str.cat(sep = ' ') #Takes each row of the reviews dataframe and puts it into a single string. Each review is separated with a space.\n",
    "    else: \n",
    "        article_str=articles.loc[identifier][\"text\"]\n",
    "    list_of_words = [word.lower() for word in wordpunct_tokenize(article_str) if word.lower() not in stop and word.isalpha()] #Lowercases each word in every article if it is not in the list of stop words and all the characters in the word are letters.\n",
    "    wordfreqdist = nltk.FreqDist(list_of_words)#Counts how frequent each word is.\n",
    "    mostcommon = wordfreqdist.most_common(25) #creates a list of the 25 most common words.\n",
    "    return mostcommon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF to Find the Keywords of Each Article\n",
    "### While we will be using functions to calculate the TF-IDF values of each word in an article, it would be wise to review the logic behind the functions.\n",
    "## Finding the IDF weights. \n",
    "### We will first find the most common words in the collection of articles. The more common the word, the smaller its IDF weight and consequently the smaller its TF-IDF value will be. <br>While there is a function that calculates the IDF weights for us, it would be wise to review the logic behind it. <br>The IDF [formula](http://www.tfidf.com/) is as follows: IDF(term) = log_e(Total number of documents / Number of documents with term t in it). In other words, the IDF weight for each term is calculated by taking the log of the total number of documents divided by the number of documents where the term is present. As a word appears in more documents, it becomes less helpful in differentiating between articles, and thus will have a smaller IDF weight.\n",
    "## Finding the TF weights\n",
    "###  The TF score for each word. The program will find this using the following [formula](http://www.tfidf.com/): TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). The more frequently a word appears in a document, the higher its TF score will be.\n",
    "## Finding the TF-IDF weights. \n",
    "### To find the TF-IDF weights, we will be using the following formula: TF-IDF=TF * IDF.  The words with the highest TF-IDF weights will be our keywords.\n",
    "#### This will be accomplished by following the following [guide](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.YTZxp9NKj0t). Unless otherwise states, the code will be from this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will first calculate the IDF weights for all the words in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_calculator(category):\n",
    "    cv=CountVectorizer(stop_words=stop) #Initializes a CountVectorizer.\n",
    "    word_count_vector=cv.fit_transform(articles[articles[\"category\"]==category]['text'])#Uses the CountVectorizer to count the frequency of each word in the corpus.\n",
    "    #We will initialize a TfidTransformer object, which will \"Transform a count matrix [word_count_vector] to a normalized tf or tf-idf representation.\" In other words, it will help us display the tf-idf values\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "    tfidf_transformer.fit(word_count_vector) #Fitting the transformer to our count matrix. This will allow us to find the IDF weights for each word based on their frequency.\n",
    "    #tfidf_transformer has an attribute idf_ which returns an array of the idf weights. We will use this to create a dataframe that displays the weights for each word. \n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    "    df_idf.sort_values(by=['idf_weights']) \n",
    "    return cv,word_count_vector,tfidf_transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the TF and TF-IDF weights for the words in an article.\n",
    "### We will now run a test to see how well our program does at finding the key words of an article. We will use the first article in the corpus. This article discusses a kerfuffle between Michael Howard and Peter Hain, promiment members of the Conservative party and Labor party respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(doc,category):  \n",
    "    tf_idf_vector=tfidf_transformer.transform(word_count_vector) \n",
    "    document_vector=tf_idf_vector[doc] #This finds the TF values for each word in the first document, and multiplies it by that word's IDF value.\n",
    "    feature_names=cv.get_feature_names() #This is sipmly a list of the words in the article.\n",
    "    #To create a dataframe from the values, we need to represent document_vector, which is a sparse matrix, as a dense matrix. \n",
    "    df = pd.DataFrame(document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "    df=df.sort_values(by=[\"tfidf\"],ascending=False) \n",
    "    print (\"Here is the article text: \",articles[articles[\"category\"]==category][\"text\"].iloc[doc])\n",
    "    #The program will now ask the user if they are satisifed with the keywords. \n",
    "    #If they are not, they can remove them one by one and replace them with the next highest ranking word.\n",
    "    #They can do this until they are satisfied with the keywords. \n",
    "    response=None\n",
    "    while response!=\"no\":\n",
    "        kws=list(df.head()['tfidf'].index) #Creates a list of the 5 words with the highest TF-IDF scores.\n",
    "        print (\"The keywords for the article are: \",kws)\n",
    "        response=input(\"Would you like to remove any of these keywords? If you do, the word with the next highest TF-IDF score will be selected to replace it. If you are satisfied with the keywords presented, please type 'no'. \\n\")\n",
    "        response=response.replace(\" \", \"\") #Removes spaces from the user's input\n",
    "        if response in kws: \n",
    "            #If their response was in the list,then the word is removed from the dataframe. \n",
    "            #This isn't permanent, however. Whenever this cell is ran, the dataframe is reset to its original form. \n",
    "            df=df.drop(response, axis=0)\n",
    "        elif response==\"no\":\n",
    "            print (\"Cool!\")\n",
    "        else: #If the user doesn't say 'no' or says a word that is not in the list of keywords.s\n",
    "            print (\"Sorry, I didn't catch that.\\n\")\n",
    "\n",
    "(cv, word_count_vector,tfidf_transformer)=idf_calculator(\"politics\")\n",
    "tf_idf(0,\"politics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The program does a good job of finding descriptive keywords for the article. The article discusses Howard's response to Hain calling him a mongrel. Howard says that Hain is calling him a mongrel, and has called him other names in the past, because the labor party is \"rattled by the opposition.\" The rest of the article provides a background behind the tension between these two parties. The keywords tell us who the article is about and references prominent talking points from the article. However, the 5th keyword, \"bit\" is not very descriptive, as it does not play a large role in the article. It was likely chosen due to it's large IDF value. As a result, the user has the option to remove it from the list and replace it next highest ranking word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The program asks the user for the category so that the IDF calculator knows what to filter the dataframe by. \n",
    "Different types of articles will likely have different commonly used words. This is important because it\n",
    "could offset the IDF score. A word like \"country\" would likely be common in political articles, but not as common \n",
    "in tech-related articles. As a result, the word \"country\" would have a higher IDF score if we just used political \n",
    "articles than had we used all articles. The former is better because it tells us that most political articles include\n",
    "the word \"country\", so if its common in a specific article, it won't necessarily make a good keyword. Had we used\n",
    "all articles, the IDF score (and consequently TF-IDF score) for \"country\" would be higher, making it more likely\n",
    "to be selected as a keyword for an article despite not being very descriptive.\n",
    "\"\"\"\n",
    "valid=False\n",
    "while valid==False:   \n",
    "    print (\"To view the keywords of an article, select a category, then select an article.\")\n",
    "    print (list(articles[\"category\"].unique()))\n",
    "    category=input(\"Please choose a category. \").replace(\" \", \"\").lower()\n",
    "    filtered_articles=articles[articles[\"category\"]==category].reset_index(drop=True) \n",
    "    display(filtered_articles)\n",
    "    num=input(\"Here are the available articles from this category, please select an article by its index. \").replace(\" \", \"\").lower()\n",
    "    try: \n",
    "        num=int(num)\n",
    "        (cv, word_count_vector,tfidf_transformer)=idf_calculator(category) #This calculates the IDF score for all articles in the category selected by the user. \n",
    "        tf_idf(num,category) #This obtains the TF-IDF scores for all the articles in the category the user selected.\n",
    "        valid=True\n",
    "    except:\n",
    "        print (\"Hmmm. It seems like an invalid number was provided. Please try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
